{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83cd0792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n\\nimport anndata\\nimport inspect\\nfrom torch_adata import AnnDataset\\nfrom pytorch_lightning import LightningDataModule\\nfrom torch.utils.data import DataLoader\\nimport os\\nimport torch\\nfrom typing import Union, List\\nimport numpy as np\\nfrom licorice_font import font_format\\n\\nfrom scdiffeq._core.utils import function_kwargs\\n\\nadata = anndata.read_h5ad(\\\"./adata.Weinreb2020.h5ad\\\")\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n\\nimport anndata\\nimport inspect\\nfrom torch_adata import AnnDataset\\nfrom pytorch_lightning import LightningDataModule\\nfrom torch.utils.data import DataLoader\\nimport os\\nimport torch\\nfrom typing import Union, List\\nimport numpy as np\\nfrom licorice_font import font_format\\n\\nfrom scdiffeq._core.utils import function_kwargs\\n\\nadata = anndata.read_h5ad(\\\"./adata.Weinreb2020.h5ad\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "\n",
    "import anndata\n",
    "import inspect\n",
    "from torch_adata import AnnDataset\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import torch\n",
    "from typing import Union, List\n",
    "import numpy as np\n",
    "from licorice_font import font_format\n",
    "\n",
    "from scdiffeq._core.utils import function_kwargs\n",
    "\n",
    "adata = anndata.read_h5ad(\"./adata.Weinreb2020.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "052cec25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"class SplitSize:\\n    def __init__(self, n_cells: int, n_groups: int):\\n\\n        self._n_cells = n_cells\\n        self._n_groups = n_groups\\n\\n    def _sum_norm(self, vals: Union[List, np.ndarray]) -> np.ndarray:\\n        return np.array(vals) / vals.sum()\\n\\n    def uniformly(self):\\n        div, mod = divmod(self._n_cells, self._n_groups)\\n        return [div] * (self._n_groups - 1) + [div + mod]\\n\\n    def proportioned(self, percentages=[0.8, 0.2], remainder_idx=-1):\\n\\n        percentages = self._sum_norm(np.array(percentages))\\n        split_lengths = [int(self._n_cells * pct_i) for pct_i in percentages]\\n        remainder = self._n_cells - sum(split_lengths)\\n        split_lengths[remainder_idx] += remainder\\n\\n        return split_lengths\\n\\n\\nclass CellDataManager:\\n    \\\"\\\"\\\"Data Manager at the AnnData Level.\\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        adata,\\n        use_key=\\\"X_pca\\\",\\n        groupby=\\\"Time point\\\",\\n        obs_keys=None,\\n        train_key=\\\"train\\\",\\n        val_key=\\\"val\\\",\\n        test_key=\\\"test\\\",\\n        predict_key=\\\"predict\\\",\\n        n_groups=None,\\n        train_val_percentages=[0.8, 0.2],\\n        remainder_idx=-1,\\n        predict_all=True,\\n        attr_names={\\\"obs\\\": [], \\\"aux\\\": []},\\n        one_hot=False,\\n        aux_keys=None,\\n        silent=True,\\n    ):\\n\\n        self.__config__(locals())\\n\\n    def __parse__(self, kwargs, ignore=[\\\"self\\\"], hide=[]):\\n\\n        for key, val in kwargs.items():\\n            if not key in ignore:\\n                setattr(self, key, val)\\n\\n    def __config__(self, kwargs, ignore=[\\\"self\\\"], hide=[]):\\n\\n        self.AnnDataset_kwargs = function_kwargs(\\n            func=AnnDataset, kwargs=kwargs, ignore=[\\\"adata\\\"]\\n        )\\n        self.AnnDataset_kwargs.pop(\\\"adata\\\")\\n        self.__parse__(kwargs, ignore, hide)\\n        self.df = self.adata.obs.copy()\\n        self.data_keys = self._get_keys(kwargs)\\n\\n        if not self.n_groups:\\n            self.n_groups = len(self.train_val_percentages)\\n\\n        self.split = SplitSize(self.n_cells, self.n_groups)\\n\\n        # configure train-val split if it has train but not val\\n        if self.has_train_not_val:\\n            self.n_fit = self.train_adata.shape[0]\\n            self._allocate_validation()\\n\\n    # -- supporting methods: -------------------------------------------------------------\\n    def _get_keys(self, kwargs):\\n        return {\\n            attr.strip(\\\"_key\\\"): val\\n            for attr, val in kwargs.items()\\n            if attr.endswith(\\\"key\\\")\\n        }\\n\\n    def _subset_adata(self, key):\\n        access_key = self.data_keys[key]\\n        if not hasattr(self.df, access_key):\\n            if (key == \\\"predict\\\") and (self.predict_all):\\n                self.df[access_key] = True\\n            else:\\n                print(\\\"Key: Access Key pair: {}:{} not found\\\".format(key, access_key))\\n        # else invoke split w/ requisite args\\n        return self.adata[self.df[access_key]].copy()\\n\\n    def _set_new_idx(self, df, idx, key_added):\\n\\n        tmp = np.zeros(len(df), dtype=bool)\\n        tmp[idx] = True\\n        df[key_added] = tmp.astype(bool)\\n\\n    def _allocate_validation(self, remainder_idx=-1):\\n        \\\"\\\"\\\"If validation key is not found, invoke this function. Takes the train subset\\n        adata and breaks it into non-overlapping train and validation adata subsets.\\\"\\\"\\\"\\n\\n        if not self.n_groups:\\n            self.n_groups = len(self.train_val_percentages)\\n\\n        train_adata = self.train_adata\\n\\n        n_cells = train_adata.shape[0]\\n\\n        self.data_keys[\\\"train\\\"] = \\\"fit_train\\\"\\n        self.data_keys[\\\"val\\\"] = \\\"fit_val\\\"\\n\\n        train_val_split = SplitSize(n_cells, self.n_groups)\\n\\n        if not self.train_val_percentages:\\n            n_train, n_val = train_val_split.uniformly()\\n        else:\\n            n_train, n_val = train_val_split.proportioned(\\n                percentages=self.train_val_percentages, remainder_idx=remainder_idx\\n            )\\n\\n        original_train_idx = train_adata.obs.index\\n        train_idx = np.random.choice(\\n            range(len(original_train_idx)), size=n_train, replace=False\\n        )\\n        train_cells = np.zeros(len(original_train_idx), dtype=bool)\\n        train_cells[train_idx] = True\\n        fit_train_idx = original_train_idx[train_cells]\\n        fit_val_idx = original_train_idx[~train_cells]\\n\\n        self._set_new_idx(self.df, idx=fit_train_idx.astype(int), key_added=\\\"fit_train\\\")\\n        self._set_new_idx(self.df, idx=fit_val_idx.astype(int), key_added=\\\"fit_val\\\")\\n\\n        self.adata.obs = self.df\\n\\n    def to_dataset(self, key):\\n        adata = getattr(self, \\\"{}_adata\\\".format(key))\\n        return AnnDataset(adata=adata, **self.AnnDataset_kwargs)\\n\\n    # -- Properties: ---------------------------------------------------------------------\\n    @property\\n    def cell_idx(self):\\n        return self.adata.obs.index\\n\\n    @property\\n    def n_cells(self):\\n        return self.adata.shape[0]\\n\\n    @property\\n    def n_features(self):\\n        return self.adata.shape[1]\\n\\n    @property\\n    def uniform_split(self) -> list([int, ..., int]):\\n        return self.split.uniformly()\\n\\n    @property\\n    def proportioned_split(self) -> list([int, ..., int]):\\n        return self.split.proportioned(percentages=self.train_val_percentages)\\n\\n    @property\\n    def train_adata(self):\\n        return self._subset_adata(\\\"train\\\")\\n\\n    @property\\n    def val_adata(self):\\n        return self._subset_adata(\\\"val\\\")\\n\\n    @property\\n    def test_adata(self):\\n        return self._subset_adata(\\\"test\\\")\\n\\n    @property\\n    def predict_adata(self):\\n        return self._subset_adata(\\\"predict\\\")\\n\\n    @property\\n    def has_train_not_val(self):\\n        return (hasattr(self.df, self.data_keys[\\\"train\\\"])) and (\\n            not hasattr(self.df, self.data_keys[\\\"val\\\"])\\n        )\\n\\n    @property\\n    def train_dataset(self):\\n        return self.to_dataset(\\\"train\\\")\\n\\n    @property\\n    def val_dataset(self):\\n        return self.to_dataset(\\\"val\\\")\\n\\n    @property\\n    def test_dataset(self):\\n        return self.to_dataset(\\\"test\\\")\\n\\n    @property\\n    def predict_dataset(self):\\n        return self.to_dataset(\\\"predict\\\")\";\n",
       "                var nbb_formatted_code = \"class SplitSize:\\n    def __init__(self, n_cells: int, n_groups: int):\\n\\n        self._n_cells = n_cells\\n        self._n_groups = n_groups\\n\\n    def _sum_norm(self, vals: Union[List, np.ndarray]) -> np.ndarray:\\n        return np.array(vals) / vals.sum()\\n\\n    def uniformly(self):\\n        div, mod = divmod(self._n_cells, self._n_groups)\\n        return [div] * (self._n_groups - 1) + [div + mod]\\n\\n    def proportioned(self, percentages=[0.8, 0.2], remainder_idx=-1):\\n\\n        percentages = self._sum_norm(np.array(percentages))\\n        split_lengths = [int(self._n_cells * pct_i) for pct_i in percentages]\\n        remainder = self._n_cells - sum(split_lengths)\\n        split_lengths[remainder_idx] += remainder\\n\\n        return split_lengths\\n\\n\\nclass CellDataManager:\\n    \\\"\\\"\\\"Data Manager at the AnnData Level.\\\"\\\"\\\"\\n\\n    def __init__(\\n        self,\\n        adata,\\n        use_key=\\\"X_pca\\\",\\n        groupby=\\\"Time point\\\",\\n        obs_keys=None,\\n        train_key=\\\"train\\\",\\n        val_key=\\\"val\\\",\\n        test_key=\\\"test\\\",\\n        predict_key=\\\"predict\\\",\\n        n_groups=None,\\n        train_val_percentages=[0.8, 0.2],\\n        remainder_idx=-1,\\n        predict_all=True,\\n        attr_names={\\\"obs\\\": [], \\\"aux\\\": []},\\n        one_hot=False,\\n        aux_keys=None,\\n        silent=True,\\n    ):\\n\\n        self.__config__(locals())\\n\\n    def __parse__(self, kwargs, ignore=[\\\"self\\\"], hide=[]):\\n\\n        for key, val in kwargs.items():\\n            if not key in ignore:\\n                setattr(self, key, val)\\n\\n    def __config__(self, kwargs, ignore=[\\\"self\\\"], hide=[]):\\n\\n        self.AnnDataset_kwargs = function_kwargs(\\n            func=AnnDataset, kwargs=kwargs, ignore=[\\\"adata\\\"]\\n        )\\n        self.AnnDataset_kwargs.pop(\\\"adata\\\")\\n        self.__parse__(kwargs, ignore, hide)\\n        self.df = self.adata.obs.copy()\\n        self.data_keys = self._get_keys(kwargs)\\n\\n        if not self.n_groups:\\n            self.n_groups = len(self.train_val_percentages)\\n\\n        self.split = SplitSize(self.n_cells, self.n_groups)\\n\\n        # configure train-val split if it has train but not val\\n        if self.has_train_not_val:\\n            self.n_fit = self.train_adata.shape[0]\\n            self._allocate_validation()\\n\\n    # -- supporting methods: -------------------------------------------------------------\\n    def _get_keys(self, kwargs):\\n        return {\\n            attr.strip(\\\"_key\\\"): val\\n            for attr, val in kwargs.items()\\n            if attr.endswith(\\\"key\\\")\\n        }\\n\\n    def _subset_adata(self, key):\\n        access_key = self.data_keys[key]\\n        if not hasattr(self.df, access_key):\\n            if (key == \\\"predict\\\") and (self.predict_all):\\n                self.df[access_key] = True\\n            else:\\n                print(\\\"Key: Access Key pair: {}:{} not found\\\".format(key, access_key))\\n        # else invoke split w/ requisite args\\n        return self.adata[self.df[access_key]].copy()\\n\\n    def _set_new_idx(self, df, idx, key_added):\\n\\n        tmp = np.zeros(len(df), dtype=bool)\\n        tmp[idx] = True\\n        df[key_added] = tmp.astype(bool)\\n\\n    def _allocate_validation(self, remainder_idx=-1):\\n        \\\"\\\"\\\"If validation key is not found, invoke this function. Takes the train subset\\n        adata and breaks it into non-overlapping train and validation adata subsets.\\\"\\\"\\\"\\n\\n        if not self.n_groups:\\n            self.n_groups = len(self.train_val_percentages)\\n\\n        train_adata = self.train_adata\\n\\n        n_cells = train_adata.shape[0]\\n\\n        self.data_keys[\\\"train\\\"] = \\\"fit_train\\\"\\n        self.data_keys[\\\"val\\\"] = \\\"fit_val\\\"\\n\\n        train_val_split = SplitSize(n_cells, self.n_groups)\\n\\n        if not self.train_val_percentages:\\n            n_train, n_val = train_val_split.uniformly()\\n        else:\\n            n_train, n_val = train_val_split.proportioned(\\n                percentages=self.train_val_percentages, remainder_idx=remainder_idx\\n            )\\n\\n        original_train_idx = train_adata.obs.index\\n        train_idx = np.random.choice(\\n            range(len(original_train_idx)), size=n_train, replace=False\\n        )\\n        train_cells = np.zeros(len(original_train_idx), dtype=bool)\\n        train_cells[train_idx] = True\\n        fit_train_idx = original_train_idx[train_cells]\\n        fit_val_idx = original_train_idx[~train_cells]\\n\\n        self._set_new_idx(self.df, idx=fit_train_idx.astype(int), key_added=\\\"fit_train\\\")\\n        self._set_new_idx(self.df, idx=fit_val_idx.astype(int), key_added=\\\"fit_val\\\")\\n\\n        self.adata.obs = self.df\\n\\n    def to_dataset(self, key):\\n        adata = getattr(self, \\\"{}_adata\\\".format(key))\\n        return AnnDataset(adata=adata, **self.AnnDataset_kwargs)\\n\\n    # -- Properties: ---------------------------------------------------------------------\\n    @property\\n    def cell_idx(self):\\n        return self.adata.obs.index\\n\\n    @property\\n    def n_cells(self):\\n        return self.adata.shape[0]\\n\\n    @property\\n    def n_features(self):\\n        return self.adata.shape[1]\\n\\n    @property\\n    def uniform_split(self) -> list([int, ..., int]):\\n        return self.split.uniformly()\\n\\n    @property\\n    def proportioned_split(self) -> list([int, ..., int]):\\n        return self.split.proportioned(percentages=self.train_val_percentages)\\n\\n    @property\\n    def train_adata(self):\\n        return self._subset_adata(\\\"train\\\")\\n\\n    @property\\n    def val_adata(self):\\n        return self._subset_adata(\\\"val\\\")\\n\\n    @property\\n    def test_adata(self):\\n        return self._subset_adata(\\\"test\\\")\\n\\n    @property\\n    def predict_adata(self):\\n        return self._subset_adata(\\\"predict\\\")\\n\\n    @property\\n    def has_train_not_val(self):\\n        return (hasattr(self.df, self.data_keys[\\\"train\\\"])) and (\\n            not hasattr(self.df, self.data_keys[\\\"val\\\"])\\n        )\\n\\n    @property\\n    def train_dataset(self):\\n        return self.to_dataset(\\\"train\\\")\\n\\n    @property\\n    def val_dataset(self):\\n        return self.to_dataset(\\\"val\\\")\\n\\n    @property\\n    def test_dataset(self):\\n        return self.to_dataset(\\\"test\\\")\\n\\n    @property\\n    def predict_dataset(self):\\n        return self.to_dataset(\\\"predict\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class SplitSize:\n",
    "    def __init__(self, n_cells: int, n_groups: int):\n",
    "\n",
    "        self._n_cells = n_cells\n",
    "        self._n_groups = n_groups\n",
    "\n",
    "    def _sum_norm(self, vals: Union[List, np.ndarray]) -> np.ndarray:\n",
    "        return np.array(vals) / vals.sum()\n",
    "\n",
    "    def uniformly(self):\n",
    "        div, mod = divmod(self._n_cells, self._n_groups)\n",
    "        return [div] * (self._n_groups - 1) + [div + mod]\n",
    "\n",
    "    def proportioned(self, percentages=[0.8, 0.2], remainder_idx=-1):\n",
    "\n",
    "        percentages = self._sum_norm(np.array(percentages))\n",
    "        split_lengths = [int(self._n_cells * pct_i) for pct_i in percentages]\n",
    "        remainder = self._n_cells - sum(split_lengths)\n",
    "        split_lengths[remainder_idx] += remainder\n",
    "\n",
    "        return split_lengths\n",
    "\n",
    "\n",
    "class CellDataManager:\n",
    "    \"\"\"Data Manager at the AnnData Level.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        adata,\n",
    "        use_key=\"X_pca\",\n",
    "        groupby=\"Time point\",\n",
    "        obs_keys=None,\n",
    "        train_key=\"train\",\n",
    "        val_key=\"val\",\n",
    "        test_key=\"test\",\n",
    "        predict_key=\"predict\",\n",
    "        n_groups=None,\n",
    "        train_val_percentages=[0.8, 0.2],\n",
    "        remainder_idx=-1,\n",
    "        predict_all=True,\n",
    "        attr_names={\"obs\": [], \"aux\": []},\n",
    "        one_hot=False,\n",
    "        aux_keys=None,\n",
    "        silent=True,\n",
    "    ):\n",
    "\n",
    "        self.__config__(locals())\n",
    "\n",
    "    def __parse__(self, kwargs, ignore=[\"self\"], hide=[]):\n",
    "\n",
    "        for key, val in kwargs.items():\n",
    "            if not key in ignore:\n",
    "                setattr(self, key, val)\n",
    "\n",
    "    def __config__(self, kwargs, ignore=[\"self\"], hide=[]):\n",
    "\n",
    "        self.AnnDataset_kwargs = function_kwargs(\n",
    "            func=AnnDataset, kwargs=kwargs, ignore=[\"adata\"]\n",
    "        )\n",
    "        self.AnnDataset_kwargs.pop(\"adata\")\n",
    "        self.__parse__(kwargs, ignore, hide)\n",
    "        self.df = self.adata.obs.copy()\n",
    "        self.data_keys = self._get_keys(kwargs)\n",
    "\n",
    "        if not self.n_groups:\n",
    "            self.n_groups = len(self.train_val_percentages)\n",
    "\n",
    "        self.split = SplitSize(self.n_cells, self.n_groups)\n",
    "\n",
    "        # configure train-val split if it has train but not val\n",
    "        if self.has_train_not_val:\n",
    "            self.n_fit = self.train_adata.shape[0]\n",
    "            self._allocate_validation()\n",
    "\n",
    "    # -- supporting methods: -------------------------------------------------------------\n",
    "    def _get_keys(self, kwargs):\n",
    "        return {\n",
    "            attr.strip(\"_key\"): val\n",
    "            for attr, val in kwargs.items()\n",
    "            if attr.endswith(\"key\")\n",
    "        }\n",
    "\n",
    "    def _subset_adata(self, key):\n",
    "        access_key = self.data_keys[key]\n",
    "        if not hasattr(self.df, access_key):\n",
    "            if (key == \"predict\") and (self.predict_all):\n",
    "                self.df[access_key] = True\n",
    "            else:\n",
    "                print(\"Key: Access Key pair: {}:{} not found\".format(key, access_key))\n",
    "        # else invoke split w/ requisite args\n",
    "        return self.adata[self.df[access_key]].copy()\n",
    "\n",
    "    def _set_new_idx(self, df, idx, key_added):\n",
    "\n",
    "        tmp = np.zeros(len(df), dtype=bool)\n",
    "        tmp[idx] = True\n",
    "        df[key_added] = tmp.astype(bool)\n",
    "\n",
    "    def _allocate_validation(self, remainder_idx=-1):\n",
    "        \"\"\"If validation key is not found, invoke this function. Takes the train subset\n",
    "        adata and breaks it into non-overlapping train and validation adata subsets.\"\"\"\n",
    "\n",
    "        if not self.n_groups:\n",
    "            self.n_groups = len(self.train_val_percentages)\n",
    "\n",
    "        train_adata = self.train_adata\n",
    "\n",
    "        n_cells = train_adata.shape[0]\n",
    "\n",
    "        self.data_keys[\"train\"] = \"fit_train\"\n",
    "        self.data_keys[\"val\"] = \"fit_val\"\n",
    "\n",
    "        train_val_split = SplitSize(n_cells, self.n_groups)\n",
    "\n",
    "        if not self.train_val_percentages:\n",
    "            n_train, n_val = train_val_split.uniformly()\n",
    "        else:\n",
    "            n_train, n_val = train_val_split.proportioned(\n",
    "                percentages=self.train_val_percentages, remainder_idx=remainder_idx\n",
    "            )\n",
    "\n",
    "        original_train_idx = train_adata.obs.index\n",
    "        train_idx = np.random.choice(\n",
    "            range(len(original_train_idx)), size=n_train, replace=False\n",
    "        )\n",
    "        train_cells = np.zeros(len(original_train_idx), dtype=bool)\n",
    "        train_cells[train_idx] = True\n",
    "        fit_train_idx = original_train_idx[train_cells]\n",
    "        fit_val_idx = original_train_idx[~train_cells]\n",
    "\n",
    "        self._set_new_idx(self.df, idx=fit_train_idx.astype(int), key_added=\"fit_train\")\n",
    "        self._set_new_idx(self.df, idx=fit_val_idx.astype(int), key_added=\"fit_val\")\n",
    "\n",
    "        self.adata.obs = self.df\n",
    "\n",
    "    def to_dataset(self, key):\n",
    "        adata = getattr(self, \"{}_adata\".format(key))\n",
    "        return AnnDataset(adata=adata, **self.AnnDataset_kwargs)\n",
    "\n",
    "    # -- Properties: ---------------------------------------------------------------------\n",
    "    @property\n",
    "    def cell_idx(self):\n",
    "        return self.adata.obs.index\n",
    "\n",
    "    @property\n",
    "    def n_cells(self):\n",
    "        return self.adata.shape[0]\n",
    "\n",
    "    @property\n",
    "    def n_features(self):\n",
    "        return self.adata.shape[1]\n",
    "\n",
    "    @property\n",
    "    def uniform_split(self) -> list([int, ..., int]):\n",
    "        return self.split.uniformly()\n",
    "\n",
    "    @property\n",
    "    def proportioned_split(self) -> list([int, ..., int]):\n",
    "        return self.split.proportioned(percentages=self.train_val_percentages)\n",
    "\n",
    "    @property\n",
    "    def train_adata(self):\n",
    "        return self._subset_adata(\"train\")\n",
    "\n",
    "    @property\n",
    "    def val_adata(self):\n",
    "        return self._subset_adata(\"val\")\n",
    "\n",
    "    @property\n",
    "    def test_adata(self):\n",
    "        return self._subset_adata(\"test\")\n",
    "\n",
    "    @property\n",
    "    def predict_adata(self):\n",
    "        return self._subset_adata(\"predict\")\n",
    "\n",
    "    @property\n",
    "    def has_train_not_val(self):\n",
    "        return (hasattr(self.df, self.data_keys[\"train\"])) and (\n",
    "            not hasattr(self.df, self.data_keys[\"val\"])\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def train_dataset(self):\n",
    "        return self.to_dataset(\"train\")\n",
    "\n",
    "    @property\n",
    "    def val_dataset(self):\n",
    "        return self.to_dataset(\"val\")\n",
    "\n",
    "    @property\n",
    "    def test_dataset(self):\n",
    "        return self.to_dataset(\"test\")\n",
    "\n",
    "    @property\n",
    "    def predict_dataset(self):\n",
    "        return self.to_dataset(\"predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba3ab611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 20;\n",
       "                var nbb_unformatted_code = \"class LightningAnnDataModule(LightningDataModule):\\n    def __init__(\\n        self,\\n        adata: [anndata.AnnData] = None,\\n        batch_size=2000,\\n        num_workers=os.cpu_count(),\\n        use_key=\\\"X_pca\\\",\\n        groupby=\\\"Time point\\\",\\n        obs_keys=None,\\n        train_key=\\\"train\\\",\\n        val_key=\\\"val\\\",\\n        test_key=\\\"test\\\",\\n        predict_key=\\\"predict\\\",\\n        n_groups=None,\\n        train_val_percentages=[0.8, 0.2],\\n        remainder_idx=-1,\\n        predict_all=True,\\n        attr_names={\\\"obs\\\": [], \\\"aux\\\": []},\\n        one_hot=False,\\n        aux_keys=None,\\n        silent=True,\\n    ):\\n        super(LightningAnnDataModule, self).__init__()\\n        self.save_hyperparameters(ignore=[\\\"adata\\\"])\\n        self._adata = adata\\n        self.cell_data_manager_kwargs = function_kwargs(\\n            func=CellDataManager, kwargs=locals(), ignore=[\\\"adata\\\"]\\n        )\\n        self.cell_data_manager_kwargs.pop(\\n            \\\"adata\\\"\\n        )  # TODO: seems like the above \\\"ignore\\\" arg isn't working...\\n\\n    # -- Supporting methods --------------------------------------------------------------\\n    def _return_loader(self, dataset_key):\\n        return DataLoader(\\n            getattr(self, \\\"{}_dataset\\\".format(dataset_key)),\\n            num_workers=self.hparams[\\\"num_workers\\\"],\\n            batch_size=self.hparams[\\\"batch_size\\\"],\\n        )\\n\\n    # -- Properties: ---------------------------------------------------------------------\\n    @property\\n    def adata(self):\\n\\n        if isinstance(self._adata, anndata.AnnData):\\n            return self._adata\\n        elif isinstance(self.hparams[\\\"h5ad_path\\\"], str):\\n            return anndata.read_h5ad(self.hparams[\\\"h5ad_path\\\"])\\n        print(\\\"Pass adata or h5ad_path\\\")\\n\\n    @property\\n    def n_cells(self):\\n        return self.adata.shape[0]\\n\\n    @property\\n    def n_features(self):\\n        return self.adata.shape[1]\\n\\n    @property\\n    def n_dims(self):\\n        return self.adata.obsm[self.hparams[\\\"use_key\\\"]].shape[1]\\n\\n    @property\\n    def batch_size(self):\\n        if not self.hparams[\\\"batch_size\\\"]:\\n            return int(self.n_cells / 10)\\n        return self.hparams[\\\"batch_size\\\"]\\n\\n    # -- Standard methods: ---------------------------------------------------------------\\n    def prepare_data(self):\\n        self.data = CellDataManager(self.adata, **self.cell_data_manager_kwargs)\\n\\n    def setup(self, stage=None):\\n\\n        if stage in [\\\"fit\\\", \\\"train\\\", \\\"val\\\"]:\\n            self.train_dataset = self.data.train_dataset\\n            self.val_dataset = self.data.val_dataset\\n\\n        elif stage == \\\"test\\\":\\n            self.test_dataset = self.data.test_dataset\\n        elif stage in [None, \\\"predict\\\"]:\\n            self.predict_dataset = self.data.predict_dataset\\n        else:\\n            print(\\n                \\\"CURRENT STAGE: {} - no suitable subset found during `LightningDataModule.setup()`\\\".format(\\n                    stage\\n                )\\n            )\\n\\n    # -- Required DataLoader methods: ----------------------------------------------------\\n    def train_dataloader(self):\\n        return self._return_loader(\\\"train\\\")\\n\\n    def val_dataloader(self):\\n        return self._return_loader(\\\"val\\\")\\n\\n    def test_dataloader(self):\\n        return self._return_loader(\\\"test\\\")\\n\\n    def predict_dataloader(self):\\n        return self._return_loader(\\\"predict\\\")\\n\\n    def __repr__(self):\\n        return \\\"\\u26a1 {} \\u26a1\\\".format(font_format(\\\"LightningAnnDataModule\\\", [\\\"PURPLE\\\"]))\";\n",
       "                var nbb_formatted_code = \"class LightningAnnDataModule(LightningDataModule):\\n    def __init__(\\n        self,\\n        adata: [anndata.AnnData] = None,\\n        batch_size=2000,\\n        num_workers=os.cpu_count(),\\n        use_key=\\\"X_pca\\\",\\n        groupby=\\\"Time point\\\",\\n        obs_keys=None,\\n        train_key=\\\"train\\\",\\n        val_key=\\\"val\\\",\\n        test_key=\\\"test\\\",\\n        predict_key=\\\"predict\\\",\\n        n_groups=None,\\n        train_val_percentages=[0.8, 0.2],\\n        remainder_idx=-1,\\n        predict_all=True,\\n        attr_names={\\\"obs\\\": [], \\\"aux\\\": []},\\n        one_hot=False,\\n        aux_keys=None,\\n        silent=True,\\n    ):\\n        super(LightningAnnDataModule, self).__init__()\\n        self.save_hyperparameters(ignore=[\\\"adata\\\"])\\n        self._adata = adata\\n        self.cell_data_manager_kwargs = function_kwargs(\\n            func=CellDataManager, kwargs=locals(), ignore=[\\\"adata\\\"]\\n        )\\n        self.cell_data_manager_kwargs.pop(\\n            \\\"adata\\\"\\n        )  # TODO: seems like the above \\\"ignore\\\" arg isn't working...\\n\\n    # -- Supporting methods --------------------------------------------------------------\\n    def _return_loader(self, dataset_key):\\n        return DataLoader(\\n            getattr(self, \\\"{}_dataset\\\".format(dataset_key)),\\n            num_workers=self.hparams[\\\"num_workers\\\"],\\n            batch_size=self.hparams[\\\"batch_size\\\"],\\n        )\\n\\n    # -- Properties: ---------------------------------------------------------------------\\n    @property\\n    def adata(self):\\n\\n        if isinstance(self._adata, anndata.AnnData):\\n            return self._adata\\n        elif isinstance(self.hparams[\\\"h5ad_path\\\"], str):\\n            return anndata.read_h5ad(self.hparams[\\\"h5ad_path\\\"])\\n        print(\\\"Pass adata or h5ad_path\\\")\\n\\n    @property\\n    def n_cells(self):\\n        return self.adata.shape[0]\\n\\n    @property\\n    def n_features(self):\\n        return self.adata.shape[1]\\n\\n    @property\\n    def n_dims(self):\\n        return self.adata.obsm[self.hparams[\\\"use_key\\\"]].shape[1]\\n\\n    @property\\n    def batch_size(self):\\n        if not self.hparams[\\\"batch_size\\\"]:\\n            return int(self.n_cells / 10)\\n        return self.hparams[\\\"batch_size\\\"]\\n\\n    # -- Standard methods: ---------------------------------------------------------------\\n    def prepare_data(self):\\n        self.data = CellDataManager(self.adata, **self.cell_data_manager_kwargs)\\n\\n    def setup(self, stage=None):\\n\\n        if stage in [\\\"fit\\\", \\\"train\\\", \\\"val\\\"]:\\n            self.train_dataset = self.data.train_dataset\\n            self.val_dataset = self.data.val_dataset\\n\\n        elif stage == \\\"test\\\":\\n            self.test_dataset = self.data.test_dataset\\n        elif stage in [None, \\\"predict\\\"]:\\n            self.predict_dataset = self.data.predict_dataset\\n        else:\\n            print(\\n                \\\"CURRENT STAGE: {} - no suitable subset found during `LightningDataModule.setup()`\\\".format(\\n                    stage\\n                )\\n            )\\n\\n    # -- Required DataLoader methods: ----------------------------------------------------\\n    def train_dataloader(self):\\n        return self._return_loader(\\\"train\\\")\\n\\n    def val_dataloader(self):\\n        return self._return_loader(\\\"val\\\")\\n\\n    def test_dataloader(self):\\n        return self._return_loader(\\\"test\\\")\\n\\n    def predict_dataloader(self):\\n        return self._return_loader(\\\"predict\\\")\\n\\n    def __repr__(self):\\n        return \\\"\\u26a1 {} \\u26a1\\\".format(font_format(\\\"LightningAnnDataModule\\\", [\\\"PURPLE\\\"]))\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class SplitSize:\n",
    "    def __init__(self, n_cells: int, n_groups: int):\n",
    "\n",
    "        self._n_cells = n_cells\n",
    "        self._n_groups = n_groups\n",
    "\n",
    "    def _sum_norm(self, vals: Union[List, np.ndarray]) -> np.ndarray:\n",
    "        return np.array(vals) / vals.sum()\n",
    "\n",
    "    def uniformly(self):\n",
    "        div, mod = divmod(self._n_cells, self._n_groups)\n",
    "        return [div] * (self._n_groups - 1) + [div + mod]\n",
    "\n",
    "    def proportioned(self, percentages=[0.8, 0.2], remainder_idx=-1):\n",
    "\n",
    "        percentages = self._sum_norm(np.array(percentages))\n",
    "        split_lengths = [int(self._n_cells * pct_i) for pct_i in percentages]\n",
    "        remainder = self._n_cells - sum(split_lengths)\n",
    "        split_lengths[remainder_idx] += remainder\n",
    "\n",
    "        return split_lengths\n",
    "\n",
    "\n",
    "class CellDataManager:\n",
    "    \"\"\"Data Manager at the AnnData Level.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        adata,\n",
    "        use_key=\"X_pca\",\n",
    "        groupby=\"Time point\",\n",
    "        obs_keys=None,\n",
    "        train_key=\"train\",\n",
    "        val_key=\"val\",\n",
    "        test_key=\"test\",\n",
    "        predict_key=\"predict\",\n",
    "        n_groups=None,\n",
    "        train_val_percentages=[0.8, 0.2],\n",
    "        remainder_idx=-1,\n",
    "        predict_all=True,\n",
    "        attr_names={\"obs\": [], \"aux\": []},\n",
    "        one_hot=False,\n",
    "        aux_keys=None,\n",
    "        silent=True,\n",
    "    ):\n",
    "\n",
    "        self.__config__(locals())\n",
    "\n",
    "    def __parse__(self, kwargs, ignore=[\"self\"], hide=[]):\n",
    "\n",
    "        for key, val in kwargs.items():\n",
    "            if not key in ignore:\n",
    "                setattr(self, key, val)\n",
    "\n",
    "    def __config__(self, kwargs, ignore=[\"self\"], hide=[]):\n",
    "\n",
    "        self.AnnDataset_kwargs = function_kwargs(\n",
    "            func=AnnDataset, kwargs=kwargs, ignore=[\"adata\"]\n",
    "        )\n",
    "        self.AnnDataset_kwargs.pop(\"adata\")\n",
    "        self.__parse__(kwargs, ignore, hide)\n",
    "        self.df = self.adata.obs.copy()\n",
    "        self.data_keys = self._get_keys(kwargs)\n",
    "\n",
    "        if not self.n_groups:\n",
    "            self.n_groups = len(self.train_val_percentages)\n",
    "\n",
    "        self.split = SplitSize(self.n_cells, self.n_groups)\n",
    "\n",
    "        # configure train-val split if it has train but not val\n",
    "        if self.has_train_not_val:\n",
    "            self.n_fit = self.train_adata.shape[0]\n",
    "            self._allocate_validation()\n",
    "\n",
    "    # -- supporting methods: -------------------------------------------------------------\n",
    "    def _get_keys(self, kwargs):\n",
    "        return {\n",
    "            attr.strip(\"_key\"): val\n",
    "            for attr, val in kwargs.items()\n",
    "            if attr.endswith(\"key\")\n",
    "        }\n",
    "\n",
    "    def _subset_adata(self, key):\n",
    "        access_key = self.data_keys[key]\n",
    "        if not hasattr(self.df, access_key):\n",
    "            if (key == \"predict\") and (self.predict_all):\n",
    "                self.df[access_key] = True\n",
    "            else:\n",
    "                print(\"Key: Access Key pair: {}:{} not found\".format(key, access_key))\n",
    "        # else invoke split w/ requisite args\n",
    "        return self.adata[self.df[access_key]].copy()\n",
    "\n",
    "    def _set_new_idx(self, df, idx, key_added):\n",
    "\n",
    "        tmp = np.zeros(len(df), dtype=bool)\n",
    "        tmp[idx] = True\n",
    "        df[key_added] = tmp.astype(bool)\n",
    "\n",
    "    def _allocate_validation(self, remainder_idx=-1):\n",
    "        \"\"\"If validation key is not found, invoke this function. Takes the train subset\n",
    "        adata and breaks it into non-overlapping train and validation adata subsets.\"\"\"\n",
    "\n",
    "        if not self.n_groups:\n",
    "            self.n_groups = len(self.train_val_percentages)\n",
    "\n",
    "        train_adata = self.train_adata\n",
    "\n",
    "        n_cells = train_adata.shape[0]\n",
    "\n",
    "        self.data_keys[\"train\"] = \"fit_train\"\n",
    "        self.data_keys[\"val\"] = \"fit_val\"\n",
    "\n",
    "        train_val_split = SplitSize(n_cells, self.n_groups)\n",
    "\n",
    "        if not self.train_val_percentages:\n",
    "            n_train, n_val = train_val_split.uniformly()\n",
    "        else:\n",
    "            n_train, n_val = train_val_split.proportioned(\n",
    "                percentages=self.train_val_percentages, remainder_idx=remainder_idx\n",
    "            )\n",
    "\n",
    "        original_train_idx = train_adata.obs.index\n",
    "        train_idx = np.random.choice(\n",
    "            range(len(original_train_idx)), size=n_train, replace=False\n",
    "        )\n",
    "        train_cells = np.zeros(len(original_train_idx), dtype=bool)\n",
    "        train_cells[train_idx] = True\n",
    "        fit_train_idx = original_train_idx[train_cells]\n",
    "        fit_val_idx = original_train_idx[~train_cells]\n",
    "\n",
    "        self._set_new_idx(self.df, idx=fit_train_idx.astype(int), key_added=\"fit_train\")\n",
    "        self._set_new_idx(self.df, idx=fit_val_idx.astype(int), key_added=\"fit_val\")\n",
    "\n",
    "        self.adata.obs = self.df\n",
    "\n",
    "    def to_dataset(self, key):\n",
    "        adata = getattr(self, \"{}_adata\".format(key))\n",
    "        return AnnDataset(adata=adata, **self.AnnDataset_kwargs)\n",
    "\n",
    "    # -- Properties: ---------------------------------------------------------------------\n",
    "    @property\n",
    "    def cell_idx(self):\n",
    "        return self.adata.obs.index\n",
    "\n",
    "    @property\n",
    "    def n_cells(self):\n",
    "        return self.adata.shape[0]\n",
    "\n",
    "    @property\n",
    "    def n_features(self):\n",
    "        return self.adata.shape[1]\n",
    "\n",
    "    @property\n",
    "    def uniform_split(self) -> list([int, ..., int]):\n",
    "        return self.split.uniformly()\n",
    "\n",
    "    @property\n",
    "    def proportioned_split(self) -> list([int, ..., int]):\n",
    "        return self.split.proportioned(percentages=self.train_val_percentages)\n",
    "\n",
    "    @property\n",
    "    def train_adata(self):\n",
    "        return self._subset_adata(\"train\")\n",
    "\n",
    "    @property\n",
    "    def val_adata(self):\n",
    "        return self._subset_adata(\"val\")\n",
    "\n",
    "    @property\n",
    "    def test_adata(self):\n",
    "        return self._subset_adata(\"test\")\n",
    "\n",
    "    @property\n",
    "    def predict_adata(self):\n",
    "        return self._subset_adata(\"predict\")\n",
    "\n",
    "    @property\n",
    "    def has_train_not_val(self):\n",
    "        return (hasattr(self.df, self.data_keys[\"train\"])) and (\n",
    "            not hasattr(self.df, self.data_keys[\"val\"])\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def train_dataset(self):\n",
    "        return self.to_dataset(\"train\")\n",
    "\n",
    "    @property\n",
    "    def val_dataset(self):\n",
    "        return self.to_dataset(\"val\")\n",
    "\n",
    "    @property\n",
    "    def test_dataset(self):\n",
    "        return self.to_dataset(\"test\")\n",
    "\n",
    "    @property\n",
    "    def predict_dataset(self):\n",
    "        return self.to_dataset(\"predict\")\n",
    "    \n",
    "class LightningAnnDataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        adata: [anndata.AnnData] = None,\n",
    "        batch_size=2000,\n",
    "        num_workers=os.cpu_count(),\n",
    "        use_key=\"X_pca\",\n",
    "        groupby=\"Time point\",\n",
    "        obs_keys=None,\n",
    "        train_key=\"train\",\n",
    "        val_key=\"val\",\n",
    "        test_key=\"test\",\n",
    "        predict_key=\"predict\",\n",
    "        n_groups=None,\n",
    "        train_val_percentages=[0.8, 0.2],\n",
    "        remainder_idx=-1,\n",
    "        predict_all=True,\n",
    "        attr_names={\"obs\": [], \"aux\": []},\n",
    "        one_hot=False,\n",
    "        aux_keys=None,\n",
    "        silent=True,\n",
    "    ):\n",
    "        super(LightningAnnDataModule, self).__init__()\n",
    "        self.save_hyperparameters(ignore=[\"adata\"])\n",
    "        self._adata = adata\n",
    "        self.cell_data_manager_kwargs = function_kwargs(\n",
    "            func=CellDataManager, kwargs=locals(), ignore=[\"adata\"]\n",
    "        )\n",
    "        self.cell_data_manager_kwargs.pop(\n",
    "            \"adata\"\n",
    "        )  # TODO: seems like the above \"ignore\" arg isn't working...\n",
    "\n",
    "    # -- Supporting methods --------------------------------------------------------------\n",
    "    def _return_loader(self, dataset_key):\n",
    "        return DataLoader(\n",
    "            getattr(self, \"{}_dataset\".format(dataset_key)),\n",
    "            num_workers=self.hparams[\"num_workers\"],\n",
    "            batch_size=self.hparams[\"batch_size\"],\n",
    "        )\n",
    "\n",
    "    # -- Properties: ---------------------------------------------------------------------\n",
    "    @property\n",
    "    def adata(self):\n",
    "\n",
    "        if isinstance(self._adata, anndata.AnnData):\n",
    "            return self._adata\n",
    "        elif isinstance(self.hparams[\"h5ad_path\"], str):\n",
    "            return anndata.read_h5ad(self.hparams[\"h5ad_path\"])\n",
    "        print(\"Pass adata or h5ad_path\")\n",
    "\n",
    "    @property\n",
    "    def n_cells(self):\n",
    "        return self.adata.shape[0]\n",
    "\n",
    "    @property\n",
    "    def n_features(self):\n",
    "        return self.adata.shape[1]\n",
    "\n",
    "    @property\n",
    "    def n_dims(self):\n",
    "        return self.adata.obsm[self.hparams[\"use_key\"]].shape[1]\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        if not self.hparams[\"batch_size\"]:\n",
    "            return int(self.n_cells / 10)\n",
    "        return self.hparams[\"batch_size\"]\n",
    "\n",
    "    # -- Standard methods: ---------------------------------------------------------------\n",
    "    def prepare_data(self):\n",
    "        self.data = CellDataManager(self.adata, **self.cell_data_manager_kwargs)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        if stage in [\"fit\", \"train\", \"val\"]:\n",
    "            self.train_dataset = self.data.train_dataset\n",
    "            self.val_dataset = self.data.val_dataset\n",
    "\n",
    "        elif stage == \"test\":\n",
    "            self.test_dataset = self.data.test_dataset\n",
    "        elif stage in [None, \"predict\"]:\n",
    "            self.predict_dataset = self.data.predict_dataset\n",
    "        else:\n",
    "            print(\n",
    "                \"CURRENT STAGE: {} - no suitable subset found during `LightningDataModule.setup()`\".format(\n",
    "                    stage\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # -- Required DataLoader methods: ----------------------------------------------------\n",
    "    def train_dataloader(self):\n",
    "        return self._return_loader(\"train\")\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self._return_loader(\"val\")\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self._return_loader(\"test\")\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return self._return_loader(\"predict\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"⚡ {} ⚡\".format(font_format(\"LightningAnnDataModule\", [\"PURPLE\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30aa38d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"class LightningDataModuleConfiguration:\\n    def __init__(\\n        self,\\n        adata: [anndata.AnnData] = None,\\n        batch_size=2000,\\n        num_workers=os.cpu_count(),\\n        use_key=\\\"X_pca\\\",\\n        groupby=\\\"Time point\\\",\\n        obs_keys=None,\\n        train_key=\\\"train\\\",\\n        val_key=\\\"val\\\",\\n        test_key=\\\"test\\\",\\n        predict_key=\\\"predict\\\",\\n        n_groups=None,\\n        train_val_percentages=[0.8, 0.2],\\n        remainder_idx=-1,\\n        predict_all=True,\\n        attr_names={\\\"obs\\\": [], \\\"aux\\\": []},\\n        one_hot=False,\\n        aux_keys=None,\\n        silent=True,\\n    ):\\n\\n        kwargs = function_kwargs(LightningAnnDataModule, locals())\\n        kwargs.pop(\\\"adata\\\")\\n\\n        self._LightningDataModule = LightningAnnDataModule(adata, **kwargs)\\n\\n    @property\\n    def LightningDataModule(self):\\n        return self._LightningDataModule\";\n",
       "                var nbb_formatted_code = \"class LightningDataModuleConfiguration:\\n    def __init__(\\n        self,\\n        adata: [anndata.AnnData] = None,\\n        batch_size=2000,\\n        num_workers=os.cpu_count(),\\n        use_key=\\\"X_pca\\\",\\n        groupby=\\\"Time point\\\",\\n        obs_keys=None,\\n        train_key=\\\"train\\\",\\n        val_key=\\\"val\\\",\\n        test_key=\\\"test\\\",\\n        predict_key=\\\"predict\\\",\\n        n_groups=None,\\n        train_val_percentages=[0.8, 0.2],\\n        remainder_idx=-1,\\n        predict_all=True,\\n        attr_names={\\\"obs\\\": [], \\\"aux\\\": []},\\n        one_hot=False,\\n        aux_keys=None,\\n        silent=True,\\n    ):\\n\\n        kwargs = function_kwargs(LightningAnnDataModule, locals())\\n        kwargs.pop(\\\"adata\\\")\\n\\n        self._LightningDataModule = LightningAnnDataModule(adata, **kwargs)\\n\\n    @property\\n    def LightningDataModule(self):\\n        return self._LightningDataModule\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class SplitSize:\n",
    "    def __init__(self, n_cells: int, n_groups: int):\n",
    "\n",
    "        self._n_cells = n_cells\n",
    "        self._n_groups = n_groups\n",
    "\n",
    "    def _sum_norm(self, vals: Union[List, np.ndarray]) -> np.ndarray:\n",
    "        return np.array(vals) / vals.sum()\n",
    "\n",
    "    def uniformly(self):\n",
    "        div, mod = divmod(self._n_cells, self._n_groups)\n",
    "        return [div] * (self._n_groups - 1) + [div + mod]\n",
    "\n",
    "    def proportioned(self, percentages=[0.8, 0.2], remainder_idx=-1):\n",
    "\n",
    "        percentages = self._sum_norm(np.array(percentages))\n",
    "        split_lengths = [int(self._n_cells * pct_i) for pct_i in percentages]\n",
    "        remainder = self._n_cells - sum(split_lengths)\n",
    "        split_lengths[remainder_idx] += remainder\n",
    "\n",
    "        return split_lengths\n",
    "\n",
    "\n",
    "class CellDataManager:\n",
    "    \"\"\"Data Manager at the AnnData Level.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        adata,\n",
    "        use_key=\"X_pca\",\n",
    "        groupby=\"Time point\",\n",
    "        obs_keys=None,\n",
    "        train_key=\"train\",\n",
    "        val_key=\"val\",\n",
    "        test_key=\"test\",\n",
    "        predict_key=\"predict\",\n",
    "        n_groups=None,\n",
    "        train_val_percentages=[0.8, 0.2],\n",
    "        remainder_idx=-1,\n",
    "        predict_all=True,\n",
    "        attr_names={\"obs\": [], \"aux\": []},\n",
    "        one_hot=False,\n",
    "        aux_keys=None,\n",
    "        silent=True,\n",
    "    ):\n",
    "\n",
    "        self.__config__(locals())\n",
    "\n",
    "    def __parse__(self, kwargs, ignore=[\"self\"], hide=[]):\n",
    "\n",
    "        for key, val in kwargs.items():\n",
    "            if not key in ignore:\n",
    "                setattr(self, key, val)\n",
    "\n",
    "    def __config__(self, kwargs, ignore=[\"self\"], hide=[]):\n",
    "\n",
    "        self.AnnDataset_kwargs = function_kwargs(\n",
    "            func=AnnDataset, kwargs=kwargs, ignore=[\"adata\"]\n",
    "        )\n",
    "        self.AnnDataset_kwargs.pop(\"adata\")\n",
    "        self.__parse__(kwargs, ignore, hide)\n",
    "        self.df = self.adata.obs.copy()\n",
    "        self.data_keys = self._get_keys(kwargs)\n",
    "\n",
    "        if not self.n_groups:\n",
    "            self.n_groups = len(self.train_val_percentages)\n",
    "\n",
    "        self.split = SplitSize(self.n_cells, self.n_groups)\n",
    "\n",
    "        # configure train-val split if it has train but not val\n",
    "        if self.has_train_not_val:\n",
    "            self.n_fit = self.train_adata.shape[0]\n",
    "            self._allocate_validation()\n",
    "\n",
    "    # -- supporting methods: -------------------------------------------------------------\n",
    "    def _get_keys(self, kwargs):\n",
    "        return {\n",
    "            attr.strip(\"_key\"): val\n",
    "            for attr, val in kwargs.items()\n",
    "            if attr.endswith(\"key\")\n",
    "        }\n",
    "\n",
    "    def _subset_adata(self, key):\n",
    "        access_key = self.data_keys[key]\n",
    "        if not hasattr(self.df, access_key):\n",
    "            if (key == \"predict\") and (self.predict_all):\n",
    "                self.df[access_key] = True\n",
    "            else:\n",
    "                print(\"Key: Access Key pair: {}:{} not found\".format(key, access_key))\n",
    "        # else invoke split w/ requisite args\n",
    "        return self.adata[self.df[access_key]].copy()\n",
    "\n",
    "    def _set_new_idx(self, df, idx, key_added):\n",
    "\n",
    "        tmp = np.zeros(len(df), dtype=bool)\n",
    "        tmp[idx] = True\n",
    "        df[key_added] = tmp.astype(bool)\n",
    "\n",
    "    def _allocate_validation(self, remainder_idx=-1):\n",
    "        \"\"\"If validation key is not found, invoke this function. Takes the train subset\n",
    "        adata and breaks it into non-overlapping train and validation adata subsets.\"\"\"\n",
    "\n",
    "        if not self.n_groups:\n",
    "            self.n_groups = len(self.train_val_percentages)\n",
    "\n",
    "        train_adata = self.train_adata\n",
    "\n",
    "        n_cells = train_adata.shape[0]\n",
    "\n",
    "        self.data_keys[\"train\"] = \"fit_train\"\n",
    "        self.data_keys[\"val\"] = \"fit_val\"\n",
    "\n",
    "        train_val_split = SplitSize(n_cells, self.n_groups)\n",
    "\n",
    "        if not self.train_val_percentages:\n",
    "            n_train, n_val = train_val_split.uniformly()\n",
    "        else:\n",
    "            n_train, n_val = train_val_split.proportioned(\n",
    "                percentages=self.train_val_percentages, remainder_idx=remainder_idx\n",
    "            )\n",
    "\n",
    "        original_train_idx = train_adata.obs.index\n",
    "        train_idx = np.random.choice(\n",
    "            range(len(original_train_idx)), size=n_train, replace=False\n",
    "        )\n",
    "        train_cells = np.zeros(len(original_train_idx), dtype=bool)\n",
    "        train_cells[train_idx] = True\n",
    "        fit_train_idx = original_train_idx[train_cells]\n",
    "        fit_val_idx = original_train_idx[~train_cells]\n",
    "\n",
    "        self._set_new_idx(self.df, idx=fit_train_idx.astype(int), key_added=\"fit_train\")\n",
    "        self._set_new_idx(self.df, idx=fit_val_idx.astype(int), key_added=\"fit_val\")\n",
    "\n",
    "        self.adata.obs = self.df\n",
    "\n",
    "    def to_dataset(self, key):\n",
    "        adata = getattr(self, \"{}_adata\".format(key))\n",
    "        return AnnDataset(adata=adata, **self.AnnDataset_kwargs)\n",
    "\n",
    "    # -- Properties: ---------------------------------------------------------------------\n",
    "    @property\n",
    "    def cell_idx(self):\n",
    "        return self.adata.obs.index\n",
    "\n",
    "    @property\n",
    "    def n_cells(self):\n",
    "        return self.adata.shape[0]\n",
    "\n",
    "    @property\n",
    "    def n_features(self):\n",
    "        return self.adata.shape[1]\n",
    "\n",
    "    @property\n",
    "    def uniform_split(self) -> list([int, ..., int]):\n",
    "        return self.split.uniformly()\n",
    "\n",
    "    @property\n",
    "    def proportioned_split(self) -> list([int, ..., int]):\n",
    "        return self.split.proportioned(percentages=self.train_val_percentages)\n",
    "\n",
    "    @property\n",
    "    def train_adata(self):\n",
    "        return self._subset_adata(\"train\")\n",
    "\n",
    "    @property\n",
    "    def val_adata(self):\n",
    "        return self._subset_adata(\"val\")\n",
    "\n",
    "    @property\n",
    "    def test_adata(self):\n",
    "        return self._subset_adata(\"test\")\n",
    "\n",
    "    @property\n",
    "    def predict_adata(self):\n",
    "        return self._subset_adata(\"predict\")\n",
    "\n",
    "    @property\n",
    "    def has_train_not_val(self):\n",
    "        return (hasattr(self.df, self.data_keys[\"train\"])) and (\n",
    "            not hasattr(self.df, self.data_keys[\"val\"])\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def train_dataset(self):\n",
    "        return self.to_dataset(\"train\")\n",
    "\n",
    "    @property\n",
    "    def val_dataset(self):\n",
    "        return self.to_dataset(\"val\")\n",
    "\n",
    "    @property\n",
    "    def test_dataset(self):\n",
    "        return self.to_dataset(\"test\")\n",
    "\n",
    "    @property\n",
    "    def predict_dataset(self):\n",
    "        return self.to_dataset(\"predict\")\n",
    "    \n",
    "class LightningAnnDataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        adata: [anndata.AnnData] = None,\n",
    "        batch_size=2000,\n",
    "        num_workers=os.cpu_count(),\n",
    "        use_key=\"X_pca\",\n",
    "        groupby=\"Time point\",\n",
    "        obs_keys=None,\n",
    "        train_key=\"train\",\n",
    "        val_key=\"val\",\n",
    "        test_key=\"test\",\n",
    "        predict_key=\"predict\",\n",
    "        n_groups=None,\n",
    "        train_val_percentages=[0.8, 0.2],\n",
    "        remainder_idx=-1,\n",
    "        predict_all=True,\n",
    "        attr_names={\"obs\": [], \"aux\": []},\n",
    "        one_hot=False,\n",
    "        aux_keys=None,\n",
    "        silent=True,\n",
    "    ):\n",
    "        super(LightningAnnDataModule, self).__init__()\n",
    "        self.save_hyperparameters(ignore=[\"adata\"])\n",
    "        self._adata = adata\n",
    "        self.cell_data_manager_kwargs = function_kwargs(\n",
    "            func=CellDataManager, kwargs=locals(), ignore=[\"adata\"]\n",
    "        )\n",
    "        self.cell_data_manager_kwargs.pop(\n",
    "            \"adata\"\n",
    "        )  # TODO: seems like the above \"ignore\" arg isn't working...\n",
    "\n",
    "    # -- Supporting methods --------------------------------------------------------------\n",
    "    def _return_loader(self, dataset_key):\n",
    "        return DataLoader(\n",
    "            getattr(self, \"{}_dataset\".format(dataset_key)),\n",
    "            num_workers=self.hparams[\"num_workers\"],\n",
    "            batch_size=self.hparams[\"batch_size\"],\n",
    "        )\n",
    "\n",
    "    # -- Properties: ---------------------------------------------------------------------\n",
    "    @property\n",
    "    def adata(self):\n",
    "\n",
    "        if isinstance(self._adata, anndata.AnnData):\n",
    "            return self._adata\n",
    "        elif isinstance(self.hparams[\"h5ad_path\"], str):\n",
    "            return anndata.read_h5ad(self.hparams[\"h5ad_path\"])\n",
    "        print(\"Pass adata or h5ad_path\")\n",
    "\n",
    "    @property\n",
    "    def n_cells(self):\n",
    "        return self.adata.shape[0]\n",
    "\n",
    "    @property\n",
    "    def n_features(self):\n",
    "        return self.adata.shape[1]\n",
    "\n",
    "    @property\n",
    "    def n_dims(self):\n",
    "        return self.adata.obsm[self.hparams[\"use_key\"]].shape[1]\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        if not self.hparams[\"batch_size\"]:\n",
    "            return int(self.n_cells / 10)\n",
    "        return self.hparams[\"batch_size\"]\n",
    "\n",
    "    # -- Standard methods: ---------------------------------------------------------------\n",
    "    def prepare_data(self):\n",
    "        self.data = CellDataManager(self.adata, **self.cell_data_manager_kwargs)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        if stage in [\"fit\", \"train\", \"val\"]:\n",
    "            self.train_dataset = self.data.train_dataset\n",
    "            self.val_dataset = self.data.val_dataset\n",
    "\n",
    "        elif stage == \"test\":\n",
    "            self.test_dataset = self.data.test_dataset\n",
    "        elif stage in [None, \"predict\"]:\n",
    "            self.predict_dataset = self.data.predict_dataset\n",
    "        else:\n",
    "            print(\n",
    "                \"CURRENT STAGE: {} - no suitable subset found during `LightningDataModule.setup()`\".format(\n",
    "                    stage\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # -- Required DataLoader methods: ----------------------------------------------------\n",
    "    def train_dataloader(self):\n",
    "        return self._return_loader(\"train\")\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self._return_loader(\"val\")\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self._return_loader(\"test\")\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return self._return_loader(\"predict\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"⚡ {} ⚡\".format(font_format(\"LightningAnnDataModule\", [\"PURPLE\"]))\n",
    "    \n",
    "class LightningDataModuleConfiguration:\n",
    "    def __init__(\n",
    "        self,\n",
    "        adata: [anndata.AnnData] = None,\n",
    "        batch_size=2000,\n",
    "        num_workers=os.cpu_count(),\n",
    "        use_key=\"X_pca\",\n",
    "        groupby=\"Time point\",\n",
    "        obs_keys=None,\n",
    "        train_key=\"train\",\n",
    "        val_key=\"val\",\n",
    "        test_key=\"test\",\n",
    "        predict_key=\"predict\",\n",
    "        n_groups=None,\n",
    "        train_val_percentages=[0.8, 0.2],\n",
    "        remainder_idx=-1,\n",
    "        predict_all=True,\n",
    "        attr_names={\"obs\": [], \"aux\": []},\n",
    "        one_hot=False,\n",
    "        aux_keys=None,\n",
    "        silent=True,\n",
    "    ):\n",
    "\n",
    "        kwargs = function_kwargs(LightningAnnDataModule, locals())\n",
    "        kwargs.pop(\"adata\")\n",
    "\n",
    "        self._LightningDataModule = LightningAnnDataModule(adata, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def LightningDataModule(self):\n",
    "        return self._LightningDataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cefc155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 25;\n",
       "                var nbb_unformatted_code = \"lit_data = LightningAnnDataModule(adata)\";\n",
       "                var nbb_formatted_code = \"lit_data = LightningAnnDataModule(adata)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lit_data = LightningAnnDataModule(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff71006e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x7ff784199ac0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x7ff6f11a35e0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 26;\n",
       "                var nbb_unformatted_code = \"lit_data.prepare_data()\\nlit_data.setup(stage=\\\"fit\\\")\\nlit_data.train_dataloader(), lit_data.val_dataloader()\";\n",
       "                var nbb_formatted_code = \"lit_data.prepare_data()\\nlit_data.setup(stage=\\\"fit\\\")\\nlit_data.train_dataloader(), lit_data.val_dataloader()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lit_data.prepare_data()\n",
    "lit_data.setup(stage=\"fit\")\n",
    "lit_data.train_dataloader(), lit_data.val_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc81a637",
   "metadata": {},
   "source": [
    "#### Now test to see if it worls within the config class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad83f74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "⚡ \u001b[95mLightningAnnDataModule\u001b[0m ⚡"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 27;\n",
       "                var nbb_unformatted_code = \"DATA_KWARGS = {}\\nlit_data = LightningDataModuleConfiguration(adata, **DATA_KWARGS).LightningDataModule\\nlit_data\";\n",
       "                var nbb_formatted_code = \"DATA_KWARGS = {}\\nlit_data = LightningDataModuleConfiguration(adata, **DATA_KWARGS).LightningDataModule\\nlit_data\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATA_KWARGS = {}\n",
    "lit_data = LightningDataModuleConfiguration(adata, **DATA_KWARGS).LightningDataModule\n",
    "lit_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sdq]",
   "language": "python",
   "name": "conda-env-sdq-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
